# üßë‚Äç‚öñÔ∏è LLM-as-Judge en Python

Este proyecto implementa un evaluador autom√°tico de respuestas basado en LLM (Large Language Model), ideal para comparar respuestas A/B o puntuar salidas individuales con criterios como precisi√≥n y relevancia.

---

## ‚öôÔ∏è Requisitos

- Python 3.9+
- Clave API v√°lida de OpenAI y de Gemini
- Las siguientes librer√≠as:
  ```bash
  pip install -r requirements.txt

## üß© Ejecuci√≥n

- Debemos completar el archivo questions.jsonl con las preguntas y los LLMs a evaluar (actualmente solo Gemini vs ChatGPT)
- Devemos a√±adir la api key de openAi y de Gemini en el archivo ".env" situado en raiz (podemos usar .env.example como ejemplo)
- Con todo esto preparado podemos ejecutar nuestra prueba
  ```bash
  cd src
  
  python judge.py
- El resultado de esta prueba se mostrar√° en consola y se guardar√° en judge_results.jsonl 